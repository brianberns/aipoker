class KuhnCFR:
	def __init__(self, iterations, decksize, buckets):
		self.nbets = 2
		self.iterations = iterations
		self.decksize = decksize
		self.cards = np.arange(decksize)
		self.nodes = {}
		self.bet_options = 2
		self.buckets = buckets
		self.counter = 0
		self.exploit = collections.defaultdict(float)

		self.m_v = [[], []]
		self.m_pi = []
		
		self.s1 = []
		self.s2 = []
		self.s3 = []

		LR = 1e-2
		self.batch_size = 10
		self.val_nets = [DeepCFRNet(decksize, self.nbets, self.bet_options, dim=16), DeepCFRNet(decksize, self.nbets, self.bet_options, dim=16)]
		self.val_net_optims = [torch.optim.Adam(self.val_nets[0].parameters(), lr = LR), torch.optim.Adam(self.val_nets[1].parameters(), lr = LR)]

		self.strategynet = DeepCFRNet(decksize, self.nbets, self.bet_options, dim=16)
		self.strategynet_optim = torch.optim.Adam(self.strategynet.parameters(), lr = LR)

	def get_strategy(self, adv):
		normalizing_sum = 0
		strategy = np.zeros(self.bet_options)
		adv = adv[0]
		for a in range(self.bet_options):
			if adv[a] > 0:
				strategy[a] = adv[a]
			else:
				strategy[a] = 0
			normalizing_sum += strategy[a]

		for a in range(self.bet_options):
			if normalizing_sum > 0:
				strategy[a] /= normalizing_sum
			else:
				strategy[a] = 1.0/self.bet_options

		if strategy[0] == 0 and strategy[1] < 0:
			strategy[0] = 1
			strategy[1] = 0
		
		if strategy[0] < 0 and strategy[1] == 0:
			strategy[1] == 1
			strategy[0] == 0

		return strategy

	def cfr_iterations_deep(self, k = 100):
		util = np.zeros(2)
		losses = []
		for t in range(1, self.iterations + 1): 
			worst_hand_opp_bet = []
			middle_hand_act_first = []
			best_hand_opp_bet = []
			# worst_hand_opp_bet_valnetwork0 = []
			# middle_hand_act_first_valnetwork0 = []
			# best_hand_opp_bet_valnetwork0 = []
			
			# worst_hand_opp_bet_valnetwork0.append(self.val_nets[0].forward(torch.tensor([[cards]], dtype=torch.float), torch.tensor(bets).float().unsqueeze(0)))
			print('THIS IS T IN THE ITERATIONS LOOP: ', t)
			if t>0 and t%100 == 0:
				for (infoset, _, action_probs, _) in self.m_pi:
					card, bet_history = infoset
					if len(bet_history) == 1 and bet_history[0] == 1 and card == 0:
						worst_hand_opp_bet.append(action_probs[0])
					elif len(bet_history) == 1 and bet_history[0] == 1 and card == 2:
						best_hand_opp_bet.append(action_probs[1])
					elif len(bet_history) == 0 and card == 1:
						middle_hand_act_first.append(action_probs[0])


				print('LOSSES', losses)

				plt.subplot(211)
				plt.scatter(np.arange(len(worst_hand_opp_bet)), worst_hand_opp_bet, label = 'worst hand opp bet')

				plt.subplot(221)
				plt.scatter(np.arange(len(best_hand_opp_bet)), best_hand_opp_bet, label = 'best hand opp bet')

				plt.subplot(222)
				plt.scatter(np.arange(len(middle_hand_act_first)), middle_hand_act_first, label = 'middle hand act first')
				plt.xlabel('Number of samples')
				plt.legend()
				plt.ylabel('Strategy percent (all should go towards 1)')
				
				plt.subplot(212)
				plt.plot(np.arange(len(losses)), losses, label = 'losses')
				plt.legend()
				plt.show()
				

				
			for i in range(2):
				for k2 in range(k):
					random.shuffle(self.cards)
					# print('THIS IS T IN THE ITERATIONS LOOP AFTER K: ', t)
					# print('THIS IS K*T IN THE ITERATIONS LOOP: ', k2*t)
					util[i] += self.deep_cfr(self.cards[:2], [], 2, 0, i, t)
				#train theta_p from scratch on loss

				#self.val_nets[i] = DeepCFRNet(self.decksize, self.nbets, self.bet_options, dim=16)
				curr_valnet = self.val_nets[i]
				curr_memory = self.m_v[i]
				curr_optim = self.val_net_optims[i]
				loss = torch.nn.MSELoss()

				# print("------------------------------------------")
				# print("Iteration: {}, Player: {}".format(t, i))
				# print("Value memory size: {}".format(len(curr_memory)))
				# print("m_v0 size: {}".format(len(self.m_v[0])))
				# print("m_v1 size: {}".format(len(self.m_v[1])))
				# print("Strategy memory size: {}".format(len(self.m_pi)))
				# for (infoset, timestep, action_probs) in self.m_pi:
				# 	print("**************************")
				# 	print("\CFR Iteration: {}".format(timestep))
				# 	print("\tInfoset: {}".format(infoset))
				# 	print("\tAction probs: {}".format(action_probs))	
				
				# for (infoset, timestep, regrets) in curr_memory:
				# 	print("**************************")
				# 	print("\CFR Iteration: {}".format(timestep))
				# 	print("\tInfoset: {}".format(infoset))
				# 	print("\tRegrets: {}".format(regrets))	
				
#				for i in range(10000):
				print("VALUE NETWORK TRAINING FOR PLAYER {}".format(i))
				for s in range(10): #sgd iterations
					batch_loss_history = []
					#print(len(curr_memory))
					print('sgd iteration: {}'.format(s))
					for (n, mem) in enumerate(curr_memory):
						infoset, timestep, regrets = mem
						cards, history = infoset
						bets = -torch.ones(self.nbets)
						for (a, b) in enumerate(history):
							bets[a] = b / (sum(history[:a]) + 2)
						#print('training bets', bets)
						valnet_out = curr_valnet.forward(torch.tensor([[cards]], dtype=torch.float), torch.tensor(bets).float().unsqueeze(0))
						# print('training valnet out', valnet_out)
						# print('training regrets', regrets)
						# print('**********************')
						# print('timestep: ', timestep)
						# print('infoset: ', infoset)
						# print('valnet_out: ', valnet_out)
						# print('regrets: ', regrets)

						stone = loss(valnet_out, torch.tensor([regrets]).float()) #* timestep
						# print('appended to batch loss history: ', stone)
						# if np.isnan(stone.detach()):
						# 	print("Stone is nan")
						# 	print("Valnet out: {}".format(valnet_out))
						# 	print("Regrets: {}".format(regrets))
						# 	exit(1)

						# print('Batch loss appending to history: ', stone)
						batch_loss_history.append(stone)
						if n % self.batch_size == 0 and n > 0:
							#print(n)
							#print('batch loss history', batch_loss_history)
							#print('batch loss history length: ', len(batch_loss_history))
							batch_loss_history_mean = torch.stack(batch_loss_history).mean()
							# print('BATCH LOSS HISTORY MEAN', batch_loss_history_mean.item())
							losses.append(batch_loss_history_mean.item())
							# if np.isnan(batch_loss_history_mean.detach()):
							# 	print("Batch loss history is nan")
							# 	print("History: {}".format(batch_loss_history))
							# 	exit(1)
							# print("Update number: {}, Loss = {}".format(n/self.batch_size, batch_loss_history))
							curr_optim.zero_grad()
							batch_loss_history_mean.backward()
							curr_optim.step()
							batch_loss_history = []

			#train theta_pi on loss
		print('m_pi')

		# save mpi to a file
		with open('m_pi_history.txt', 'w+') as f:
			for (infoset, timestep, action_probs, player) in self.m_pi:
				f.write("{}, {}\n{}\n{}\n\n".format(player, timestep, infoset, action_probs))
		
		for p in range(100):
			batch_loss_history = []
			#print(len(self.m_pi))
			print('ITERATION OF POLICY NETWORK: ', p)
			for (n, mem) in enumerate(self.m_pi):
				infoset, timestep, action_probs, _ = mem
				cards, history = infoset
				bets = -torch.ones(self.nbets)
				for (a, b) in enumerate(history):
					bets[a] = b / (sum(history[:a]) + 2)
				strategynet_out = self.strategynet.forward(torch.tensor([[cards]], dtype=torch.float), bets.float().unsqueeze(0))
				batch_loss_history.append(timestep * loss(strategynet_out, torch.tensor([action_probs]).float()))
				if n % self.batch_size == 0 and n>0:
					batch_loss_history = torch.stack(batch_loss_history).mean()
					print('BATCH LOSS HISTORY POLICY: ', batch_loss_history)
					self.strategynet_optim.zero_grad()
					batch_loss_history.backward()
					self.strategynet_optim.step()
					batch_loss_history = []
		
		bets = -torch.ones(self.nbets)
		print('Average game value: {}'.format(util[0]/(self.iterations*k)))
		a1 = self.strategynet.forward(torch.tensor([[0]], dtype=torch.float), torch.tensor(bets).float().unsqueeze(0))
		a2 = self.strategynet.forward(torch.tensor([[1]], dtype=torch.float), torch.tensor(bets).float().unsqueeze(0))
		a3 = self.strategynet.forward(torch.tensor([[2]], dtype=torch.float), torch.tensor(bets).float().unsqueeze(0))
		bets = -torch.ones(self.nbets)
		bets[0] = 0.5
		a4 = self.strategynet.forward(torch.tensor([[2]], dtype=torch.float), torch.tensor(bets).float().unsqueeze(0))
		a5 = self.strategynet.forward(torch.tensor([[0]], dtype=torch.float), torch.tensor(bets).float().unsqueeze(0))
		print('card 0 opening action: ', a1)
		print('card 1 opening action: ', a2)
		print('card 2 opening action: ', a3)
		print('card 2 facing bet action: ', a4)
		print('card 0 facing bet action: ', a5)
		#return self.m_pi

	def brf(self, player_card, history, player_iteration, opp_reach, buckets):
		plays = len(history)
		acting_player = plays % 2
		expected_payoff = 0

		if plays >= 2: #can be terminal
			opponent_dist = np.zeros(len(opp_reach))
			opponent_dist_total = 0
			#print('opp reach', opp_reach)
			if history[-1] == 'f' or history[-1] == 'c' or (history[-1] == history[-2] == 'k'):
				for i in range(len(opp_reach)):
					opponent_dist_total += opp_reach[i] #compute sum of dist. for normalizing
				for i in range(len(opp_reach)):
					opponent_dist[i] = opp_reach[i] / opponent_dist_total
					payoff = 0
					is_player_card_higher = player_card > i
					if history[-1] == 'f': #bet fold
						if acting_player == player_iteration:
							payoff = 1
						else:
							payoff = -1
					elif history[-1] == 'c': #bet call
						if is_player_card_higher:
							payoff = 2
						else:
							payoff = -2
					elif (history[-1] == history[-2] == 'k'): #check check
						if is_player_card_higher:
							payoff = 1
						else:
							payoff = -1
					expected_payoff += opponent_dist[i] * payoff
				return expected_payoff

		d = np.zeros(2) #opponent action distribution
		d = [0, 0]

		new_opp_reach = np.zeros(len(opp_reach))
		for i in range(len(opp_reach)):
			new_opp_reach[i] = opp_reach[i]

		v = -100000
		util = np.zeros(2)
		util = [0, 0]
		w = np.zeros(2)
		w = [0, 0]

		#infoset = history

		for a in range(2):
			if acting_player != player_iteration:
				for i in range(len(opp_reach)):
					if buckets > 0:
						bucket1 = 0
						for j in range(buckets):
							if j < len(opp_reach)/buckets * (j + 1):
								bucket1 = j
								break
						infoset = str(bucket1) + history
					else:
						infoset = str(i) + history 
					if infoset not in self.nodes:
						self.nodes[infoset] = Node(2)
					strategy = self.nodes[infoset].get_average_strategy()#get_strategy_br()
					new_opp_reach[i] = opp_reach[i] * strategy[a] #update reach prob
					w[a] += new_opp_reach[i] #sum weights over all poss. of new reach

			if a == 0:
				if len(history) != 0:
					if history[-1] == 'b':
						next_history = history + 'f'
					elif history[-1] == 'k':
						next_history = history + 'k'
				else:
					next_history = history + 'k'
			elif a == 1:
				if len(history) != 0:
					if history[-1] == 'b':
						next_history = history + 'c'
					elif history[-1] == 'k':
						next_history = history + 'b'
				else:
					next_history = history + 'b'
			#print('w', w)
			#print('history', history)
			#print('next history', next_history)
			util[a] = self.brf(player_card, next_history, player_iteration, new_opp_reach, buckets)
			#print('util a', util[a])
			if (acting_player == player_iteration and util[a] > v):
				v = util[a] #this action better than previously best action
		
		if acting_player != player_iteration:
			#D_(-i) = Normalize(w) , d is action distribution that = normalized w
			d[0] = w[0] / (w[0] + w[1])
			d[1] = w[1] / (w[0] + w[1])
			v = d[0] * util[0] + d[1] * util[1]

		return v

	def do_cfr(self, cards, history, p0, p1, pot, nodes_touched):
		plays = len(history)
		acting_player = plays % 2
		opponent_player = 1 - acting_player
		self.counter += 1

		if plays >= 2:
			if history[-1] == 'f': #bet fold
				return 1
			if (history[-1] == history[-2] == 'k') or (history[-1] == 'c'): #check check or bet call, go to showdown
				if cards[acting_player] > cards[opponent_player]:
					return pot/2 #profit
				else:
					return -pot/2

		#print('infoset cards: {}'.format(cards[acting_player]))
		#print('infoset history: {}'.format(history))
		num_actions = 2

		
		if self.buckets > 0:
			bucket = int(cards[acting_player] * self.buckets/self.decksize)
			infoset = str(bucket) + history

		else:
			infoset = str(cards[acting_player]) + history

		if infoset not in self.nodes:
			self.nodes[infoset] = Node(num_actions)

		print('==INFOSET==', infoset)
		print('REGRET_SUM FOR ACTION P', self.nodes[infoset].regret_sum[0])
		print('REGRET_SUM FOR ACTION B', self.nodes[infoset].regret_sum[1])
		print('STRATEGY SUM FOR ACTION P', self.nodes[infoset].strategy_sum[0])
		print('STRATEGY SUM FOR ACTION B', self.nodes[infoset].strategy_sum[1])

		nodes_touched += 1

		if self.counter % 1000 == 0:
			br = np.zeros(2)
			opp_reach = np.zeros(self.decksize)
			for p in [0, 1]:
				for c1 in range(self.decksize):
					for c2 in range(self.decksize):
						if c1 == c2:
							opp_reach[c2] = 0
						else:
							opp_reach[c2] = 1.0/(self.decksize - 1.0)
					br[p] += self.brf(c1, '', p, opp_reach, self.buckets)
			print('Iteration number: ', self.counter)
			print('Best response player 0: ', br[0])
			print('Best response player 1: ', br[1])
			print('Exploitability: ', (br[0] + br[1]) / 2)
			self.exploit[nodes_touched] = (br[0] + br[1]) / 2

		if acting_player == 0:
			realization_weight = p0
		else:
			realization_weight = p1
		print('realization weight p0', p0)
		print('realization weight p1', p1)
		strategy = self.nodes[infoset].get_strategy(realization_weight)
		print('strategy', strategy)
		util = np.zeros(self.bet_options) #2 actions
		node_util = 0

		for a in range(num_actions):
			#print('a is: {}'.format(a))
			if a == 0:
				if len(history) != 0:
					if history[-1] == 'b':
						next_history = history + 'f'
					elif history[-1] == 'k':
						next_history = history + 'k'
				else:
					next_history = history + 'k'
			elif a == 1:
				if len(history) != 0:
					if history[-1] == 'b':
						next_history = history + 'c'
					elif history[-1] == 'k':
						next_history = history + 'b'
				else:
					next_history = history + 'b'
				pot += 1
			if acting_player == 0:
				util[a] = -self.do_cfr(cards, next_history, p0*strategy[a], p1, pot, nodes_touched)
			elif acting_player == 1: 
				 util[a] = -self.do_cfr(cards, next_history, p0, p1*strategy[a], pot, nodes_touched)
			print('util of {}'.format(a), util[a])
			node_util += strategy[a] * util[a]

		print('node util', node_util)
		for a in range(2):
			regret = util[a] - node_util
			print('regret of {}'.format(a), regret)
			if acting_player == 0:
				self.nodes[infoset].regret_sum[a] += p1 * regret
				print('regret sum increase for action {} for player 0 += p1 * regret'.format(a), p1 * regret)
			elif acting_player == 1:
				self.nodes[infoset].regret_sum[a] += p0 * regret
				print('regret sum increase for action {} for player 1 += p0 * regret'.format(a), p0 * regret)

		return node_util
	
	def deep_cfr(self, cards, history, pot, nodes_touched, traversing_player, t):
		print('THIS IS ITERATION', t)
		plays = len(history)
		acting_player = plays % 2
		opponent_player = 1 - acting_player

		if plays >= 2:
			if history[-1] == 0 and history[-2] == 1: #bet fold
				if acting_player == traversing_player:
					return 1
				else:
					return -1
			if (history[-1] == history[-2] == 0) or (history[-1] == history[-2] == 1): #check check or bet call, go to showdown
				if acting_player == traversing_player:
					if cards[acting_player] > cards[opponent_player]:
						return pot/2 #profit
					else:
						return -pot/2
				else:
					if cards[acting_player] > cards[opponent_player]:
						return -pot/2
					else:
						return pot/2

		#print('infoset cards: {}'.format(cards[acting_player]))
		#print('infoset history: {}'.format(history))
		num_actions = 2

		if self.buckets > 0:
			bucket = int(cards[acting_player] * self.buckets/self.decksize)
			infoset = str(bucket) + history

		else:
			#infoset = str(cards[acting_player]) + history
			infoset = cards[acting_player], history
			#print(cards[acting_player], history)
			infoset_str = str(cards[acting_player]) + ''.join(str(history))

		if infoset_str not in self.nodes:
			self.nodes[infoset_str] = Node(num_actions)

		bets = -torch.ones(self.nbets)
		for (i, b) in enumerate(history):
			# bets should be a list of the proportion each bet is of the pot size
			bets[i] = b / (sum(history[:i]) + 2)

		nodes_touched += 1

		#strategy = self.cfrnet.forward(torch.tensor([[[cards[traversing_player]]]]), torch.tensor(bets).float().unsqueeze(0))
		#print('strategy', strategy)
		#strategy = self.nodes[infoset].get_strategy()  
		#should come from predicted advantages using regret matching with network for the acting player

		if acting_player == traversing_player:
			util = np.zeros(self.bet_options) #2 actions
			node_util = 0
			advantages = self.val_nets[acting_player].forward(torch.tensor([[cards[acting_player]]], dtype=torch.float), torch.tensor(bets).float().unsqueeze(0))
			
			
			strategy = self.get_strategy(advantages)
			# print('TRAVERSING PLAYER')
			# print('acting player cards: ', cards[acting_player])
			# print('bet history: ', bets)
			# print('advantages: ', advantages)
			# print('strategy: ', strategy)
			# if cards[acting_player] == 0 and history == [1]:
			# 	self.s1.append(strategy[0])
			# if cards[acting_player] == 1 and history == []:
			# 	self.s2.append(strategy[0])
			# if cards[acting_player] == 2 and history == [1]:
			# 	self.s3.append(strategy[0])
			for a in range(num_actions):
				#print('a is: {}'.format(a))
				next_history = history + [a]
				pot += a
				util[a] = self.deep_cfr(cards, next_history, pot, nodes_touched, traversing_player, t)
				node_util += strategy[a] * util[a]

			action_advantages = np.zeros(num_actions)
			for a in range(num_actions):
				action_advantages[a] = util[a] - node_util
			#print("in deep cfr", t)
			print('************************')
			print('infoset of situation: ', infoset)
			print('action advantages from valnet: ', advantages)
			print('action advantages when acting player == traversing player: ', action_advantages)
			self.m_v[traversing_player].append((infoset, t, action_advantages))
			return node_util

		else: #acting_player != traversing_player
			advantages = self.val_nets[acting_player].forward(torch.tensor([[cards[acting_player]]],  dtype=torch.float), torch.tensor(bets).float().unsqueeze(0))
			strategy = self.get_strategy(advantages)
			# print('NON-TRAVERSING PLAYER')
			# print('acting player cards: ', cards[acting_player])
			# print('bet history: ', bets)
			# print('advantages: ', advantages)
			# print('strategy: ', strategy)
			action_probs = np.zeros(num_actions)
			for a in range(num_actions):
				#self.nodes[infoset].strategy_sum[a] += strategy[a]
				action_probs[a] = strategy[a]
				#insert infoset, t, strategies into strategy memory
			self.m_pi.append((infoset, t, action_probs, acting_player))
			# print('ITERATION: ', t)
			# print('M_PI: ', self.m_pi)
			util = 0
			if random.random() < strategy[0]:
				next_history = history + [0]
			else: 
				next_history = history + [1]
				pot += 1
			return self.deep_cfr(cards, next_history, pot, nodes_touched, traversing_player, t)